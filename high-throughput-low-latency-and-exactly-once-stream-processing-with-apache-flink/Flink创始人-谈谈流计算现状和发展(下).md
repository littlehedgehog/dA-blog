> 本文主要翻译自[《high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink》](https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-  stream-processing-with-apache-flink)，作为Flink发明人以及dA CEO，Kostas                                                                                                              Tzoumas对于流式处理有深入的见解。文章分为两部分，本篇文章是下半部分有关流式计算实验和数据说明。文章考虑到语言文化差异，对于部分较难理解部分加入我自己的说明。


# 给点数据呗

为了进一步说明Apache Flink™的性能，我们设计了一系列实验，用以研究Flink的吞吐，延迟以及容错机制的影响。下面所有实验均在Google Compute Engine上进行，使用30个实例，每个实例包含4核和15GB内存。所有Flink测试均使用截至7月24日的最新代码修订版进行，所有Storm测试均使用0.9.3版。如有需要，大家可以在[此处](https://github.com/dataArtisans/performance)找到用于评估的所有代码。

同时，为了更好进行横向比较，我们也提供了在Apache Storm上面运行相同程序的结果。如上一篇文章所介绍，Apache Storm曾经是最广泛使用的流式处理系统之一，它核心机制是实现了"记录确认(译注: record acknowledgements)"以及"微批处理(译注: mini-batch)"。(后者是通过Storm Trident实现)

## 吞吐

我们在Google云上，组建30台机器总计120Core的集群，用来测量Flink和Storm对两个不同程序的吞吐量。 第一个程序是并行流式grep任务，它在流中搜索包含与正则表达式匹配的字符串的事件。grep应用程序具备的特征能够让grep非常容易做到并行处理，并且基于流分区进行伸缩。

![图片](https://data-artisans.com/wp-content/uploads/2015/08/grep_throughput-1024x548.png)

在Flink集群上，我们可以看到Flink每核每秒平均有150万条记录的持续吞吐量，这使Flink集群总吞吐量达到每秒1.82亿记录。Flink的计算延迟为零，因为作业不涉及网络，也不涉及微批处理。而当打开Flink的容错机制，并设定每5秒做一次快照，可以看到的是Flink吞吐量有轻微降低（小于2％）。可以说，Flink优秀的容错机制并不会引入任何计算延迟。

在Storm集群上，当我们关闭记录确认机制（即没有任何数据准确性保证），Storm处理吞吐能力是每核每秒约82000条记录，99%的处理延迟在10毫秒以内，因此整个Storm集群的总吞吐量为每秒57万条记录。当启用记录确认（即保证数据至少处理一次，at-least-once）时，Storm的吞吐量降至每核每秒4700条记录，同时Storm的延迟也增加到30-120毫秒。 接下来，我们使用Storm Trident，其微批量大小为200000个元组。 Trident实现了每核每秒75000条记录的吞吐量（集群总吞吐量与Storm原生处理机制在关闭容错机制情况下整体吞吐量大致相同）。然而，这个集群吞吐性能是以3000毫秒的延迟（99%的百分位数是3000ms）为代价换来的。

我们看到Flink的吞吐量比Trident高出20倍以上，吞吐量比Storm高300倍，在如此高吞吐情况下，Flink还保证了计算延迟几乎为零。另外，我们还看到，Flink规避了微批处理模型，因此Flink的高吞吐量并不会以牺牲延迟为代价 。 Flink可以将Source节点和Sink节点链接(Chain)在一起，从而将数据在Flink内部传递优化为在单个JVM里面交换下数据记录的句柄而已。

之后，我们还进行了如下实验，将计算集群核心数量从40个扩展到120个。因为Grep程序是一个易于并行处理的逻辑，因此所有框架处理能力理论上都能够做到线性扩展。现在让我们再做一个稍加不同的实验，它按数据业务主键执行流分组，从而实现通过网络对数据流进行混洗(Shuffle)。同样，我们仍然在30台机器的集群中运行此作业，集群硬件系统配置与以前完全相同。Flink集群的吞吐能力如下，当关闭快照检查点是每核每秒大约720000条记录，当打开快照检查点后降至690000条记录每秒。 请注意，Flink在每个检查点均备份所有Operator的状态，而Storm则完全不支持这种功能。 此示例中的状态相对较小（状态主要是计数和摘要，每个检查点每个运算符的数量小于1MB）。 Storm在打开记录确认情况下，具有每核心每秒约2600条记录的吞吐能力。

![Grouping的性能](https://data-artisans.com/wp-content/uploads/2015/08/throughput_grouping.png)

## 延迟

一个大数据系统能否处理大规模数据量肯定至关重要。 但在流式处理系统中，另外一个特别重要的点在于计算延迟。 对于欺诈检测或IT安全等应用程序，在毫秒级别能够进行事件处理意味着可以避免业务损失，一套流式处理系统最低只能优化到100毫秒的延迟通常意味着前述问题只能在业务损失发生的事后才能发现，而此时的问题发现对于我们避免业务损失实际上意义已经不大了。

当应用程序开发人员评估一套流式处理系统性能延迟时，他们通常需要一个底层处理系统告之他们延迟分布情况。我们设计一个实验，测量业务主键聚合场景下作业的延迟分布情况，该作业由于存在主键聚合，因此需要流式系统内部数据跨网络混洗。下图显示了延迟分布情况，包括延迟中位数、延迟第90%位数、第95%分位数、第99%分位数（所谓第99%分位数的50毫秒延迟，意味着99％的元素延迟不超过50毫秒）。

![时间分布](https://data-artisans.com/wp-content/uploads/2015/08/latency_grouping-1024x577.png)

值得注意的是，Flink在以最大吞吐量运行时，其处理中值延迟为26毫秒，第99百分位延迟为51毫秒，这意味着99％的延迟都低于51毫秒。 当我们打开Flink的检查点机制（打开exactly-once的状态更新保证）并没有增加明显的延迟。但此时，我们确实看到处于较高百分位数的延迟增加，有观察到的延迟大约为150毫秒。这类情况主要原因是流在对齐所消耗的延迟，此时的Operator在等待接收所有输入的Barrier(译注: 关于Barrier部分，请参考https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.html这篇文章)。Storm具有非常低的中值延迟（1毫秒），并且第99百分位延迟也是51毫秒。

对于大多数应用程序而言，重要的是能够在可接受的延迟中维持较高吞吐量，延迟的具体需求取决于特定应用程序的业务要求。 在Flink中，用户可以使用称之为**Buffer Timeout**的机制来调整延迟。为了提高性能，Flink的Operator在将数据发送到下一个Operator之前会将数据暂存在缓冲区。通过指定缓冲区超时时间，例如设定10毫秒，我们可以告诉Flink当面临 1)缓冲区已满 2)10毫秒已过 的情况下发送当前缓冲区所有的数据。 通常来说设定一个较低的缓冲区超时间将优化流式处理的延迟，但随之而来的是会降低相应的计算吞吐量。 在上面的实验中，我们将Flink缓冲区超时设置为50毫秒，这解释了为什么第99百分位的是50毫秒。

我们再进一步解释下延迟是如何影响Flink中的吞吐量。因为设定较低的延迟时间将不可避免地意味着缓存数据的减少，因此必然会产生一定的吞吐量成本。 下图显示了不同缓冲区超时时间设置下的Flink吞吐量情况。 该实验再次使用流记录分组聚合的作业。

![flink-buffer-timtout](https://data-artisans.com/wp-content/uploads/2015/08/buffer_timeout.png)

如果指定缓冲区超时时间为零，处理的记录会立即转发到下游的Operator而不会进行缓冲。 如此的延迟优化，Flink可以实现0毫秒的中值延迟，以及99%延迟在20毫秒以下。当然，随之带来相应的吞吐量是每核每秒24500记录处理能力。当我们增加缓冲区超时时间，我们会看到延迟增加，吞吐量会同时增加，直到达到吞吐量峰值，缓冲区填充速度超过缓冲区超时到期时间。例如，设置50毫秒的缓冲区超时时间，Flink系统将达到每核每秒750000条记录的峰值吞吐量，99%的处理延迟在50毫秒以下。

## 正确性与恢复开销

我们的最后一个实验开始测试做快照检查点机制的正确性保证以及故障恢复的开销。我们需要运行一个需要强一致性的流式程序，并定期杀死工作节点。

这个实验的测试程序受到网络安全/入侵检测等用例的启发，并使用规则来检查事件序列的有效性（例如，身份验证令牌，登录，服务交互）。该程序从Kafka并行地读取事件流，并通过生成一些实体标识（例如，IP地址或用户ID）作为主键进行分组。 对于每个事件，流式处理程序会根据一些业务规则校验事件的顺序性（例如，“服务交互”必须在“登录”之前）。 对于乱序，或者说无效的事件序列，程序会发布警报。如果没有exactly-once的语义保证，人为制造的故障将直接产生无效的事件序列并导致程序发布错误警报。

同样，我们在一个30节点的集群中运行这个程序，其中“YARN chaos monkey”进程每5分钟将随机杀死一个的YARN容器。 我们保留备用worker（即Flink中的TaskManagers），这样系统可以在发生故障后立即获取到新资源并运行作业，而无需等待YARN启动新的容器。接着，Flink将重新启动失败的worker并在后台将其加入到Flink调度集群中，以确保备用worker始终可用。

为了保证能够模拟出我们期待的效果，我们开发了并发的数据生成器，这些生成器将以每核每秒30000的速率生成数据，并将数据推送到Kafka。 下图显示了数据生成器的速率（红线），以及从Kafka读取事件并使用规则验证事件序列的Flink作业的吞吐量（蓝线）。

![kafka-throughput](https://data-artisans.com/wp-content/uploads/2015/08/kafka-throughput-no-red-1024x504.png)


# 后续规划

在dataArtisans公司，我们正在研究Flink流处理的几个重大功能，并希望很快将它们作为下一个Flink版本的一部分提供。（译注: 这篇文章写于2015年，因此下面作者提到的Flink Feature实际上已经全部实现）。

## 高可用性
现在，Flink的主节点（称为JobManager）是单点故障。 我们正在引入具有备用主节点的主高可用性，该节点使用Apache Zookeeper进行主/备用协调。

## EventTime和Watermark
我们正在向Flink添加按事件时间处理乱序事件的能力，即创建记录时的时间戳而不是处理时的时间戳，以及Watermark的引入。

## 改进运行作业的监控
我们正在开发一个完全重新设计的管理接口，该接口提供用户可以在运行时观察底层运行细节，并获取统计信息，例如累加器(accumulators)。 如果您对此感兴趣并希望了解有关Apache Flink™，Google Cloud Dataflow以及其他技术和实际用例的更多信息，请注册Flink Forward 2015。
